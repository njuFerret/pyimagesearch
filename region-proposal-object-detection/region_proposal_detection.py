# -----------------------------
#   USAGE
# -----------------------------
# python region_proposal_detection.py --image beagle.png

# -----------------------------
#   IMPORTS
# -----------------------------
# Import the necessary packages
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.applications import imagenet_utils
from tensorflow.keras.preprocessing.image import img_to_array
from imutils.object_detection import non_max_suppression
import numpy as np
import argparse
import cv2


# -----------------------------
#   IMPORTS
# -----------------------------
def selective_search(image, method="fast"):
    # Initialize the OpenCV's selective search implementation and set the input image
    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()
    ss.setBaseImage(image)
    # Check to see if the *fast* but *less accurate* version of selective search is going to be used
    if method == "fast":
        ss.switchToSelectiveSearchFast()
    # Otherwise, use the *slow* but *more accurate* version of selective search
    else:
        ss.switchToSelectiveSearchQuality()
    # Run selective search on the input image
    rects = ss.process()
    # Return the region proposal bounding boxes
    return rects


# Construct the argument parser and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--image", required=True, help="Path to the input image")
ap.add_argument("-m", "--method", type=str, default="fast", choices=["fast", "quality"], help="Selective search method")
ap.add_argument("-c", "--conf", type=float, default=0.9,
                help="Minimum probability to consider a classification/detection")
ap.add_argument("-f", "--filter", type=str, default=None, help="Comma separated list of ImageNet labels to filter on")
args = vars(ap.parse_args())

# Grab the label filters command line argument
labelFilters = args["filter"]

# If the label filter is not empty, break it into a list
if labelFilters is not None:
    labelFilters = labelFilters.lower().split(",")

# Load the ResNet from disk (with weights pre-trained on ImageNet)
print("[INFO] Loading ResNet...")
model = ResNet50(weights="imagenet")

# Load the input image from disk and grab its dimensions
image = cv2.imread(args["image"])
(H, W) = image.shape[:2]

# Run the selective search on the input image
print("[INFO] Performing selective search with '{}' method...".format(args["method"]))
rects = selective_search(image, method=args["method"])
print("[INFO] Found '{}'".format(rects) + "regions with '{}' method of selective search!".format(args["method"]))

# Initialize the list of regions proposals that are going to be used for classifying alongside their associated
# bounding boxes
proposals = []
bbox = []

# Loop over the region proposal bounding box coordinates generated by running selective search
for (x, y, w, h) in rects:
    # If the width or height of the region is less than 10% of the image width or height, ignore it
    # (i.e, filter out small objects that are likely false-positives)
    if w/float(W) < 0.1 or h/float(H) < 0.1:
        continue
    # Extract the region from the input image, convert it from BGR to RGB channel ordering, and then resize it to
    # 224x224 ( the input dimensions required by training the pre-trained CNN)
    roi = image[y:y + h, x:x + w]
    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
    roi = cv2.resize(roi, (224, 224))
    # Further preprocess by the ROI
    roi = img_to_array(roi)
    roi = preprocess_input(roi)
    # Update the proposals and bounding boxes lists
    proposals.append(roi)
    bbox.append((x, y, w, h))

# Convert the proposals list into a NumPy array and show its dimensions
proposals = np.array(proposals)
print("[INFO] Proposals shape: {}".format(proposals.shape))

# Classify each one of the proposals ROIs using the ResNet and then decode the predictions
print("[INFO] Classifying proposals...")
preds = model.predict(proposals)
preds = imagenet_utils.decode_predictions(preds, top=1)

# Initialize a dictionary that maps class labels (keys) to any bounding box associated with that label (values)
labels = {}

# Loop over the predictions
for (i, p) in enumerate(preds):
    # Grab the prediction information for the current region proposal
    (imageNetID, label, prob) = p[0]
    # Only if the label filters are not empty *and* the label does not exits in the list then ignore it
    if labelFilters is not None and label not in labelFilters:
        continue
    # Filter out weak detections by ensuring the predicted probability is greater than the minimum probability
    if prob >= args["conf"]:
        # Grab the bounding box associated with the prediction and convert the coordinates
        (x, y, w, h) = bbox[i]
        box = (x, y, x + w, y + h)
        # Grab the list of predictions for the label and add the bounding box + probability to the list
        L = labels.get(label, [])
        L.append((box, prob))
        labels[label] = L

# Loop over the labels for each of the detected objects in the image
for label in labels.keys():
    # Clone the original image in order to draw on it
    print("[INFO] Showing results for '{}'".format(label))
    clone = image.copy()
    # Loop over the all the bounding boxes for the current label
    for (box, prob) in labels[label]:
        # Draw on the bounding box on the image
        (startX, startY, endX, endY) = box
        cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)
    # Show the results *before* applying non-maximum suppression, then clone the image again in order to display the
    # results *after* applying non-maximum suppression
    cv2.imshow("Before", clone)
    clone = image.copy()
    # Extract the bounding box and the associated prediction probabilities, then apply non-maximum suppression
    boxes = np.array([p[0] for p in labels[label]])
    proba = np.array([p[1] for p in labels[label]])
    boxes = non_max_suppression(boxes, proba)
    # Loop over all the bounding boxes that were kept after applying the non-maximum suppression
    for (startX, startY, endX, endY) in boxes:
        # Draw the bounding box and label on the image
        cv2.rectangle(clone, (startX, startY), (endX, endY), (0, 255, 0), 2)
        y = startY - 10 if startY - 10 > 10 else startY + 10
        cv2.putText(clone, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)
    # Show the output after applying non-maximum suppression
    cv2.imshow("After", clone)
    cv2.waitKey(0)

# Destroy all windows
cv2.destroyAllWindows()

